{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In any machine learning task, cleaning or preprocessing the data is as important as model building if not more. And when it comes to unstructured data like text, this process is even more important.\n",
    "\n",
    "# Objective of this code is to understand the various text preprocessing steps with examples.\n",
    "\n",
    "# Some of the common text preprocessing / cleaning steps are:\n",
    "\n",
    "# * Sentence Tokenizer\n",
    "# * Word Tokenizer\n",
    "# * Porter Stemming \n",
    "# * Snowball Stemming\n",
    "# * Lemmatization\n",
    "# * Removal of Punctuations\n",
    "# * Removal of Stopwords\n",
    "# * Removal of Frequent words\n",
    "# * Removal of Rare words\n",
    "# * Bag Of Words (Word Embeddings)\n",
    "\n",
    "# So these are the different types of text preprocessing steps which we can do on text data. But we need not do all of these all the times. We need to carefully choose the preprocessing steps based on our use case since that also play an important role.\n",
    "\n",
    "# For example, in sentiment analysis use case, we need not remove the emojis or emoticons as it will convey some important information about the sentiment. Similarly we need to decide based on our use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OpenAI is an artificial intelligence (AI) research laboratory consisting of the for-profit corporation OpenAI LP and its parent company, the non-profit OpenAI Inc.', 'The company, considered a competitor to DeepMind, conducts research in the field of AI with the stated goal of promoting and developing friendly AI in a way that benefits humanity as a whole.']\n"
     ]
    }
   ],
   "source": [
    "#Sentence Tokenizer\n",
    "import nltk\n",
    "input_str = \"OpenAI is an artificial intelligence (AI) research laboratory consisting of the for-profit corporation OpenAI LP and its parent company, the non-profit OpenAI Inc. The company, considered a competitor to DeepMind, conducts research in the field of AI with the stated goal of promoting and developing friendly AI in a way that benefits humanity as a whole.\"\n",
    "from nltk.tokenize import sent_tokenize\n",
    "tokens = sent_tokenize(input_str)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OpenAI', 'is', 'an', 'artificial', 'intelligence', '(', 'AI', ')', 'research', 'laboratory', 'consisting', 'of', 'the', 'for-profit', 'corporation', 'OpenAI', 'LP', 'and', 'its', 'parent', 'company', ',', 'the', 'non-profit', 'OpenAI', 'Inc', '.', 'The', 'company', ',', 'considered', 'a', 'competitor', 'to', 'DeepMind', ',', 'conducts', 'research', 'in', 'the', 'field', 'of', 'AI', 'with', 'the', 'stated', 'goal', 'of', 'promoting', 'and', 'developing', 'friendly', 'AI', 'in', 'a', 'way', 'that', 'benefits', 'humanity', 'as', 'a', 'whole', '.']\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenizer\n",
    "import nltk\n",
    "input_str = \"OpenAI is an artificial intelligence (AI) research laboratory consisting of the for-profit corporation OpenAI LP and its parent company, the non-profit OpenAI Inc. The company, considered a competitor to DeepMind, conducts research in the field of AI with the stated goal of promoting and developing friendly AI in a way that benefits humanity as a whole.\"\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(input_str)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original : Natural, New: natur\n",
      "Original : language, New: languag\n",
      "Original : processing, New: process\n",
      "Original : NLP, New: nlp\n",
      "Original : linguistics, New: linguist\n",
      "Original : computer, New: comput\n",
      "Original : science, New: scienc\n",
      "Original : artificial, New: artifici\n",
      "Original : intelligence, New: intellig\n",
      "Original : concerned, New: concern\n",
      "Original : interactions, New: interact\n",
      "Original : computers, New: comput\n",
      "Original : language, New: languag\n",
      "Original : computers, New: comput\n",
      "Original : analyze, New: analyz\n",
      "Original : large, New: larg\n",
      "Original : amounts, New: amount\n",
      "Original : natural, New: natur\n",
      "Original : language, New: languag\n"
     ]
    }
   ],
   "source": [
    "# Porter Stemming \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer_1 = PorterStemmer()\n",
    "input_str = \"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\"\n",
    "input_str = word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    porter_stemmed_token = stemmer_1.stem(word)\n",
    "    if word != porter_stemmed_token:\n",
    "        print(f\"Original : {word}, New: {porter_stemmed_token}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original : Natural, New: natur\n",
      "Original : language, New: languag\n",
      "Original : processing, New: process\n",
      "Original : NLP, New: nlp\n",
      "Original : linguistics, New: linguist\n",
      "Original : computer, New: comput\n",
      "Original : science, New: scienc\n",
      "Original : artificial, New: artifici\n",
      "Original : intelligence, New: intellig\n",
      "Original : concerned, New: concern\n",
      "Original : interactions, New: interact\n",
      "Original : computers, New: comput\n",
      "Original : language, New: languag\n",
      "Original : computers, New: comput\n",
      "Original : analyze, New: analyz\n",
      "Original : large, New: larg\n",
      "Original : amounts, New: amount\n",
      "Original : natural, New: natur\n",
      "Original : language, New: languag\n"
     ]
    }
   ],
   "source": [
    "# Snowball Stemming\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer_2 = SnowballStemmer(\"english\")\n",
    "input_str = \"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\"\n",
    "input_str = word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    snowball_stemmed_token = stemmer_2.stem(word)\n",
    "    if word != snowball_stemmed_token:\n",
    "        print(f\"Original : {word}, New: {snowball_stemmed_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization with pos Verb for Played: play\n",
      "Lemmatization with pos Noun for Played: played\n",
      "['The', 'ideal', 'characteristic', 'of', 'artificial', 'intelligence', 'is', 'it', 'ability', 'to', 'rationalize', 'and', 'take', 'action', 'that', 'have', 'the', 'best', 'chance', 'of', 'achieving', 'a', 'specific', 'goal.']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization with Part of Speech Tags\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "pos_verb = lemma.lemmatize(word='played',pos=wordnet.VERB)\n",
    "pos_noun = lemma.lemmatize(word='played',pos=wordnet.NOUN)\n",
    "print(f\"Lemmatization with pos Verb for Played: {pos_verb}\")\n",
    "print(f\"Lemmatization with pos Noun for Played: {pos_noun}\")\n",
    "\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    return [lemma.lemmatize(word) for word in text.split()]\n",
    "\n",
    "sentence = \"The ideal characteristic of artificial intelligence is its ability to rationalize and take actions that have the best chance of achieving a specific goal.\"\n",
    "print(lemmatize_words(sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "Text before removing puntuation: My friends caught the bus without me; I was left standing around on my own.\n",
      "Text after removing puntuation: My friends caught the bus without me I was left standing around on my own\n"
     ]
    }
   ],
   "source": [
    "# Punctuations \n",
    "import string\n",
    "Punctuation = string.punctuation\n",
    "print (Punctuation)\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', Punctuation))\n",
    "\n",
    "text_without_punctuation = remove_punctuation(\"My friends caught the bus without me; I was left standing around on my own.\")\n",
    "print(\"Text before removing puntuation: My friends caught the bus without me; I was left standing around on my own.\")\n",
    "print(f\"Text after removing puntuation: {text_without_punctuation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Stop Words: {\"she's\", 'had', 'during', 'other', 'over', 'hadn', 'am', 'have', 'yourself', 'were', 'with', 's', 'once', \"should've\", 'wasn', 'but', 'few', 't', 'didn', 'then', 'on', 'more', 'mustn', 'he', 'y', 'd', \"hadn't\", 'than', \"wasn't\", 'by', 'now', 'same', 'ain', \"aren't\", 'into', 'does', 'until', 'needn', 'too', 'been', 'itself', 'each', 'ourselves', 'are', 'if', 'for', 'it', 'couldn', 'hers', 'those', 'here', 'my', 'that', 'some', 'me', 'an', \"you'll\", 'between', 'yourselves', 'doing', 'there', 'not', 'mightn', 'm', \"shouldn't\", 'after', 'won', 'very', 'shan', 'from', 'all', 'our', 'do', 'yours', \"shan't\", 'ours', 'when', 'wouldn', 'shouldn', 'where', 'which', \"hasn't\", 'both', 'having', 'can', \"don't\", 'such', 'down', 'at', 'most', 'the', 'below', 'own', 'isn', 'you', \"isn't\", 'ma', \"needn't\", 'because', 'they', 'these', 'up', 'nor', 'before', 'further', 'through', 'has', 'while', 'who', 'don', \"mightn't\", \"won't\", \"weren't\", \"mustn't\", \"doesn't\", 'so', 'in', 'your', 'as', 'we', 'theirs', \"you've\", 'she', 'this', 'themselves', 'only', 'under', 'be', 'him', 'weren', 'will', 're', 'was', 'what', 'did', 'no', 'above', 'herself', 'why', \"you'd\", 'just', \"couldn't\", 'or', 'of', 'his', \"that'll\", 'off', 'hasn', 'about', 'to', 'their', 'i', 'himself', 'her', 'whom', 'out', 'any', 'o', 'and', 'aren', \"you're\", 've', \"didn't\", 'should', \"haven't\", \"it's\", 'against', 'being', 'them', 'a', 'again', 'how', 'doesn', 'its', \"wouldn't\", 'll', 'myself', 'is', 'haven'}\n",
      "Sentence after removing Stop Words: ['My', 'friends', 'caught', 'bus', 'without', ';', 'I', 'left', 'standing', 'around', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "Stop_Words = set(stopwords.words('english'))\n",
    "print(f\"List of Stop Words: {Stop_Words}\")\n",
    "\n",
    "\n",
    "sentence = \"My friends caught the bus without me; I was left standing around on my own.\"\n",
    "\n",
    "words = nltk.word_tokenize(sentence)\n",
    "without_stop_words = [word for word in words if not word in Stop_Words]\n",
    "print(f\"Sentence after removing Stop Words: {without_stop_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 5),\n",
       " ('happy', 2),\n",
       " ('because', 2),\n",
       " ('felt', 1),\n",
       " ('saw', 1),\n",
       " ('the', 1),\n",
       " ('others', 1),\n",
       " ('were', 1),\n",
       " ('and', 1),\n",
       " ('knew', 1),\n",
       " ('should', 1),\n",
       " ('feel', 1),\n",
       " ('happy,', 1),\n",
       " ('but', 1),\n",
       " ('wasn’t', 1),\n",
       " ('really', 1),\n",
       " ('happy.', 1)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frequent words\n",
    "\n",
    "from collections import Counter\n",
    "Sentence = \"I felt happy because I saw the others were happy and because I knew I should feel happy, but I wasn’t really happy.\"\n",
    "Count = Counter()\n",
    "for text in Sentence.split():\n",
    "    Count[text] += 1\n",
    "        \n",
    "Count.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'should', 'really', 'happy.', 'wasn’t', 'feel', 'knew', 'were', 'and', 'happy,', 'but'}\n"
     ]
    }
   ],
   "source": [
    "# Rare words\n",
    "# Rare words can be found by the least frequent words. We can use the previous code. \n",
    "no_of_rare_words = 10\n",
    "RAREWORDS = set([w for (w, wc) in Count.most_common()[:-no_of_rare_words-1:-1]])\n",
    "print(RAREWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Everyday': 1,\n",
       " 'learning': 1,\n",
       " 'NLP': 1,\n",
       " 'is': 1,\n",
       " 'a': 1,\n",
       " 'good': 1,\n",
       " 'practice': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bag Of Words (Word Embeddings)\n",
    "Sentences = [\"Welcome to my blog. Use my blog to start learning NLP\", \"Everyday learning NLP is a good practice\"]\n",
    "\n",
    "{\"welcome\":1, \"to\":2, \"my\":2, \"blog\":2, \"Use\":1, \"start\":1, \"learning\":1 , \"NLP\":1} # Sentence 1\n",
    "\n",
    "{\"Everyday\":1, \"learning\":1, \"NLP\":1, \"is\":1, \"a\":1, \"good\":1, \"practice\":1} # Sentence 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'welcome': 1,\n",
       " 'to': 2,\n",
       " 'my': 2,\n",
       " 'blog': 2,\n",
       " 'Use': 1,\n",
       " 'start': 1,\n",
       " 'learning': 2,\n",
       " 'NLP': 2,\n",
       " 'Everyday': 1,\n",
       " 'is': 1,\n",
       " 'a': 1,\n",
       " 'good': 1,\n",
       " 'practice': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count of words in both the sentence\n",
    "{\"welcome\":1, \"to\":2, \"my\":2, \"blog\":2, \"Use\":1, \"start\":1, \"learning\":2, \"NLP\":2, \"Everyday\":1, \"is\":1, \"a\":1, \"good\":1, \"practice\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 0 0 1 2 1 0 1 2 1 1]\n",
      " [0 1 1 1 1 0 1 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "Vectorized_Sentence = vectorizer.fit_transform(Sentences)\n",
    "print(Vectorized_Sentence.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
